---
title: "Depth Analysis in Native Space"
output: 
  html_document:
    toc: true
    code_folding: hide
---

## Analytical strategy


**Quantifying brain activity in each cortical depth bin**\
We are interested in detecting differences in the profile of layer-specific activity during the observation of either scrambled or unscrambled movies representing goal-directed actions.

By profile of activity we mean the brain activity - estimated using GLM - at different cortical depth (bin). The basic unit of estimation is therefore the activity at a given cortical depth bin.


**Unreliability of a common anatomical template (e.g. MNI)**\
Estimating this quantity at the group level using a template proved unfeasible at the least, since moderate degrees-of-freedom elastic transformation do not - and should not - perfectly align the morphology of every subject, while aggressive transformation disrupt the single-subject morphology.

In addition, voxel-level functional localization is already hard to establish for conventional spatial resolution fMRI (2-3mm), and it is therefore practically unfeasible for layer-specific fMRI (~0.6 mm resolution).

**Group analysis based on Juelich regions**\
This prompted me to devise a different strategy to estimate group-level effects _without_ relying on a common anatomical template. This strategy is based on two insights:

- carrying out layer-specific estimation in the native space is the best strategy to preserve layer-specific information, which would be lost in interpolations to other-subject spaces, and likely compromised in the transformation to the anatomical scan - as it would require upsampling.
- activity can be quantified at the level of atlas-based cytoarchitectonically defined maps, for which the degree of precision in template-to-subject registration needs not to be extremely precise

In this notebook, I estimate layer-specific activity for each subject, condition, run, in each of the cytoarchitectonically defined regions in the Juelich atlas. The basic data which needs to be available is therefore, for each subject:

- thresholded Z stat for each contrast and each run in the native space
- cortical depth map for each run registered in the native space
- Juelich maps for each run registered in the native space

**Proportion of voxels across layers instead of Z value**\
In a previous analysis I considered, for each cortical depth bin in each Juelich region, the average Z value across runs for each subject in either contrasts (Motion and Scrambled), and took this quantity to the group level.

The choice of using the thresholded zstat instead of the raw zstat is twofold:

- thresholded zstat are already present on the map of cortical depth only (rather than in the entire brain - or partial brain FOV), which is where I want to estimate my statistic
- thresholded zstat have already been corrected for MCP at the subject level

While these properties still justify the use of thresholded Z values at the single run level in the native space to _select_ the voxels to consider, I then realized that the _mean thresholded Z value across runs_ might not be the best quantity for our purposes. 

The Z value reflecting the significant activity _in each contrast_ is actually a binary (significant or not) rather than a continous estimator. For using the thresholded Z value in order to assess difference one could maybe consider the Z value resulting from the contrast of Motion vs. Scrambled. However the latter led to poor results and mostly scattered small clusters of nonsignificant voxels: it looks like the two contrasts activate mostly the same regions, and this comparison does not have the appropriate granularity to highlight layer-based differences.

I then thought to use the count of voxels in each bin, but also this quantity is not a good one, since (1) it relies on a same Z threshold which can lead to different number of voxels for each contrast and - especially - (2) it does not consider the single bin in relation to the entire cortical depth profile.

Finally I resolved to use a metric which appear not to have the limitations above, that is, for each contrast, **the proportion of above Z threshold voxels in each bin with respect to that in other bins**. 

This metric has the advantages of:

1. being independent from the absolute (thresholded) values of Z voxels across contrasts
2. being independent from the absolute number of voxels in each bin/region for each contrast
3. relating the effect within each bin to the effect in all the other bins

Therefore in the present analysis the metric that will be taken to the group level for each subject, juelich ROI and bin is the average proportion of suprathreshold voxels across runs.


**User-defined criteria**\
These choices require the user to input some thresholding criteria:

- It might be the case - as it actually is - that not for every subject the two contrasts yield significant clusters of voxels in each run. 

- Even so, the number of voxels in a given ROI might still be very small to be considered interesting: keep in mind that we are looking at proportions: a ROI with only one voxel in a given ROI would give a very distorted view of the proportion of active voxels across the whole cortical depth.

- Even if both of the above conditions are satisfied, it might be so only in 2-3 subjects, which would make the statistical inference on the comparison between the two contrasts completely invalid.

For these reasons, I have to input the following thresholding parameters manually and in this order:

1. for a given subject, the minimum amount of voxels in each Julich ROI (note that the results have already been corrected with GRF, therefore they can be assimilated to clusters)

2. for a given subject, the minimum amount of runs in which a given ROI featured suprathreshold voxels

3. at the group level, minimum amount of subjects having suprathreshold voxels in each ROI

In the present notebook, these criteria must be input below in the cell `Run for all subjects and all contrasts`. Just before the table and plot of the results, there is a summary of the parameters used.

These criteria are indeed quite stringent, since they are based on the single-subject and single-run corrected results. However I believe that this can result in a more robust result.

Note: I did not develop an interactive version of this machine since for every variation of a parameter, it needs to process about 300 high-resolution nifti images, which takes about 20 seconds. 


**Analytic complexity** (i.e. why this pipeline is so long)\
The engineering of the method is quite complex, since I have:

- 2 conditions for each run
- 8 runs for each subject
- N Juelich ROIs for each subject/run
- 8 cortical depth maps for each subject (one for each run)

A (simplified) overview of the complexity of the data is provided in the interactive tree map below.






